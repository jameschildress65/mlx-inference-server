# Vision Model Support - Setup Guide
**MLX Inference Server - Multimodal/Vision-Language Models**

---

## Overview

The MLX Inference Server supports vision-language models (VLMs) that can process both text and images. This guide covers setup for vision support on Apple Silicon.

**Key Features:**
- Process images alongside text prompts
- OpenAI-compatible vision API
- Optimized for Apple Silicon (Metal GPU acceleration)
- Isolated environment prevents dependency conflicts

---

## System Requirements

**Minimum:**
- Apple Silicon Mac (M1/M2/M3/M4)
- 16GB RAM (for smallest models)
- macOS 13.0+ (Ventura or newer)
- Python 3.12+

**Recommended for Best Performance:**
- 32GB+ RAM
- M3/M4 series (faster Metal GPU)

---

## Vision Environment Setup

Vision models require a separate virtual environment (`venv-vision`) with specific dependency versions that differ from text-only models.

### 1. Create Vision Virtual Environment

```bash
# From mlx-inference-server directory
python3 -m venv venv-vision
```

### 2. Install Vision Dependencies

```bash
# Install MLX vision libraries
venv-vision/bin/pip install mlx-vlm pillow transformers>=4.44.0,<5.0

# Install server dependencies
venv-vision/bin/pip install setproctitle pyyaml posix-ipc
```

**Why separate environment?**
- Text models use `transformers 5.0.0rc1` (optimized for MLX text)
- Vision models require `transformers 4.x` (mlx-vlm compatibility)
- Isolated environments prevent conflicts

### 3. Verify Installation

```bash
venv-vision/bin/python -c "import mlx_vlm; print('Vision environment ready!')"
```

Expected output: `Vision environment ready!`

---

## Recommended Vision Models

### For MacBook Air M4 (32GB) or Similar

**Best Choice: Qwen2.5-VL-7B-Instruct-4bit** â­

**Specifications:**
- **Memory Usage:** ~5.8 GB
- **Performance:** ~90-95 tokens/second
- **Download Size:** ~4 GB
- **Model ID:** `mlx-community/Qwen2.5-VL-7B-Instruct-4bit`

**Why 7B over 3B?**
- 11x faster generation (95 tok/s vs 8 tok/s)
- More consistent performance
- Better quality responses
- Only 2.3GB more memory than 3B

**Memory Headroom on 32GB:**
- Model: 5.8GB
- System: ~6GB
- Available: ~20GB (plenty of headroom)

### Model Installation

Models download automatically on first use. To pre-download:

```bash
# Set model cache location (optional)
export HF_HOME=~/.cache/huggingface

# Pre-download model (optional, happens automatically otherwise)
venv-vision/bin/python -c "
from transformers import AutoModel, AutoProcessor
model = AutoModel.from_pretrained('mlx-community/Qwen2.5-VL-7B-Instruct-4bit')
processor = AutoProcessor.from_pretrained('mlx-community/Qwen2.5-VL-7B-Instruct-4bit')
print('Model downloaded successfully!')
"
```

---

## Usage

### Start the Server

The server automatically detects vision models and uses the appropriate environment.

```bash
# Start server (same command as text-only)
./bin/mlx-inference-server-daemon.sh

# Or foreground mode
python3 mlx_inference_server.py
```

### Vision API Request Format

**OpenAI-Compatible Multimodal Format:**

```bash
curl -X POST http://localhost:11440/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/Qwen2.5-VL-7B-Instruct-4bit",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "text", "text": "What is in this image?"},
          {
            "type": "image_url",
            "image_url": {
              "url": "https://example.com/image.jpg"
            }
          }
        ]
      }
    ],
    "max_tokens": 200
  }'
```

### Image Input Options

**Option 1: HTTP(S) URL**
```json
{
  "type": "image_url",
  "image_url": {"url": "https://example.com/photo.jpg"}
}
```

**Option 2: Base64 Data URI**
```json
{
  "type": "image_url",
  "image_url": {"url": "data:image/jpeg;base64,/9j/4AAQSkZJRg..."}
}
```

**Option 3: Local File (convert to base64)**
```bash
# Convert local image to base64
base64 -i image.jpg | tr -d '\n' > image_b64.txt

# Use in request
IMAGE_B64=$(cat image_b64.txt)
curl -X POST http://localhost:11440/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"mlx-community/Qwen2.5-VL-7B-Instruct-4bit\",
    \"messages\": [{
      \"role\": \"user\",
      \"content\": [
        {\"type\": \"text\", \"text\": \"Describe this image\"},
        {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,${IMAGE_B64}\"}}
      ]
    }],
    \"max_tokens\": 300
  }"
```

### Multiple Images

```json
{
  "messages": [{
    "role": "user",
    "content": [
      {"type": "text", "text": "Compare these two images"},
      {"type": "image_url", "image_url": {"url": "https://example.com/image1.jpg"}},
      {"type": "image_url", "image_url": {"url": "https://example.com/image2.jpg"}}
    ]
  }],
  "max_tokens": 300
}
```

**Limits:**
- Maximum 5 images per request
- Maximum 10MB per image (base64 encoded)
- Supported formats: JPEG, PNG, GIF, WebP

---

## Testing Vision Setup

### Quick Test

```bash
# Test with a sample image URL
curl -X POST http://localhost:11440/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/Qwen2.5-VL-7B-Instruct-4bit",
    "messages": [{
      "role": "user",
      "content": [
        {"type": "text", "text": "What do you see?"},
        {"type": "image_url", "image_url": {"url": "https://picsum.photos/400/300"}}
      ]
    }],
    "max_tokens": 150
  }'
```

Expected response should include image description.

---

## Troubleshooting

### Error: "Vision environment not found"

**Problem:** Server can't find `venv-vision`

**Solution:**
```bash
# Create venv-vision in project root
python3 -m venv venv-vision
venv-vision/bin/pip install mlx-vlm pillow transformers>=4.44.0,<5.0
venv-vision/bin/pip install setproctitle pyyaml posix-ipc
```

### Error: "No module named 'mlx_vlm'"

**Problem:** Vision dependencies not installed

**Solution:**
```bash
venv-vision/bin/pip install mlx-vlm pillow
```

### Slow Performance (< 10 tok/s)

**Possible Causes:**
1. **Wrong model variant:** Use 4-bit quantized models (`-4bit`)
2. **Insufficient RAM:** Vision models need headroom
3. **Metal GPU not utilized:** Check Activity Monitor â†’ GPU tab

**Solution:**
- Use recommended 4-bit models
- Close memory-heavy applications
- Verify Metal GPU acceleration enabled

### Image Download Timeout

**Problem:** Large images from slow URLs

**Solution:**
- Use smaller images (< 5MB recommended)
- Use base64 for local images (bypasses download)
- Increase timeout (server config)

### Out of Memory (OOM)

**Problem:** Model + images exceed RAM

**Solution:**
- Use smaller model (consider 3B if 7B OOMs)
- Reduce number of images per request
- Close other applications
- Consider Mac Studio/Pro with more RAM

---

## Performance Expectations

### Qwen2.5-VL-7B-Instruct-4bit

**MacBook Air M4 (32GB):**
- Load time: ~6 seconds
- Generation: 90-95 tokens/second
- Memory: ~5.8 GB

**Mac Studio M4 Max (128GB):**
- Load time: ~5 seconds
- Generation: 95-100 tokens/second
- Memory: ~5.8 GB

**Mac Mini M4 (16GB):**
- Load time: ~7 seconds
- Generation: 85-90 tokens/second
- Memory: ~5.8 GB
- Note: Less headroom for images

---

## Architecture Notes

### Dual Virtual Environment

```
Project Root/
â”œâ”€â”€ venv/                   # Text-only models
â”‚   â”œâ”€â”€ transformers 5.0.0rc1
â”‚   â”œâ”€â”€ mlx-lm
â”‚   â””â”€â”€ mlx 0.22.1+
â”‚
â”œâ”€â”€ venv-vision/            # Vision models
â”‚   â”œâ”€â”€ transformers 4.57.3
â”‚   â”œâ”€â”€ mlx-vlm 0.3.9
â”‚   â”œâ”€â”€ Pillow 11.0.0
â”‚   â””â”€â”€ mlx 0.22.1+
```

**Automatic Routing:**
- Server detects model type from model ID
- `Qwen2-VL`, `llava`, `idefics` â†’ `venv-vision`
- All other models â†’ `venv` (text-only)
- Zero configuration required

### Security Features

- Private IP blocking (SSRF protection)
- Image size limits (10MB max)
- Image count limits (5 per request)
- Decompression bomb protection
- Input validation on all endpoints

---

## Next Steps

1. âœ… Set up `venv-vision`
2. âœ… Install dependencies
3. âœ… Start server
4. âœ… Test with sample image
5. ðŸš€ Build your vision-powered application

**Questions or Issues?**
- Check [README.md](../README.md) for general setup
- See [INSTALLATION-GUIDE.md](INSTALLATION-GUIDE.md) for detailed prerequisites
- Review server logs for debugging

---

**Document Version:** 1.0
**Last Updated:** 2025-12-30
**Tested Models:** Qwen2.5-VL (3B, 7B)
**Tested Hardware:** M4 Air (32GB), M4 Max (128GB)
